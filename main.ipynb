{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "from model import *\n",
    "from pretrain import *\n",
    "from utils.utils import *\n",
    "import train\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--dataset\", default='banking', type=str, help=\"The name of the dataset to train selected.\")\n",
    "\n",
    "parser.add_argument(\"--known_cls_ratio\", default=0.75, type=float, help=\"The number of known classes.\")\n",
    "\n",
    "parser.add_argument(\"--cluster_num_factor\", default=1.0, type=float, help=\"The factor (magnification) of the number of clusters K.\")\n",
    "\n",
    "parser.add_argument(\"--data_dir\", default='data', type=str,\n",
    "                    help=\"The input data dir. Should contain the .csv files (or other data files) for the task.\")\n",
    "\n",
    "parser.add_argument(\"--save_results_path\", type=str, default='outputs', help=\"The path to save results.\")\n",
    "\n",
    "parser.add_argument(\"--pretrain_dir\", default='pretrain_models', type=str,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "\n",
    "parser.add_argument(\"--train_dir\", default='train_models', type=str,\n",
    "                    help=\"The output directory where the final model is stored in.\")\n",
    "\n",
    "parser.add_argument(\"--bert_model\", default=\"bert-base-uncased\", type=str,\n",
    "                    help=\"The path or name for the pre-trained bert model.\")\n",
    "\n",
    "parser.add_argument(\"--tokenizer\", default=\"bert-base-uncased\", type=str,\n",
    "                    help=\"The path or name for the tokenizer\")\n",
    "\n",
    "parser.add_argument(\"--max_seq_length\", default=None, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                            \"than this will be truncated, sequences shorter will be padded.\")\n",
    "\n",
    "parser.add_argument(\"--feat_dim\", default=768, type=int, help=\"The feature dimension.\")\n",
    "\n",
    "parser.add_argument(\"--warmup_proportion\", default=0.1, type=float)\n",
    "\n",
    "parser.add_argument(\"--freeze_bert_parameters\", action=\"store_true\", help=\"Freeze the last parameters of BERT.\")\n",
    "\n",
    "parser.add_argument(\"--save_model\", default=True, type=str, help=\"Save trained model.\")\n",
    "\n",
    "parser.add_argument(\"--pretrain\", action=\"store_true\", help=\"Pre-train the model with labeled data.\")\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=0, help=\"Random seed for initialization.\")\n",
    "\n",
    "parser.add_argument(\"--rtr_prob\", default=0.25, type=float,\n",
    "                    help=\"Probability for random token replacement\")\n",
    "\n",
    "parser.add_argument(\"--labeled_ratio\", default=0.1, type=float,\n",
    "                    help=\"The ratio of labeled samples in the training set.\")\n",
    "\n",
    "parser.add_argument(\"--gpu_id\", type=str, default='0', help=\"Select the GPU id.\")\n",
    "\n",
    "parser.add_argument(\"--train_batch_size\", default=128, type=int,\n",
    "                    help=\"Batch size for training.\")\n",
    "\n",
    "parser.add_argument(\"--eval_batch_size\", default=128, type=int,\n",
    "                    help=\"Batch size for evaluation.\")\n",
    "\n",
    "parser.add_argument(\"--pre_wait_patient\", default=20, type=int,\n",
    "                    help=\"Patient steps for pre-training Early Stop.\")\n",
    "\n",
    "parser.add_argument(\"--num_pretrain_epochs\", default=100, type=float,\n",
    "                    help=\"The pre-training epochs.\")\n",
    "\n",
    "parser.add_argument(\"--num_train_epochs\", default=80, type=float,\n",
    "                    help=\"The training epochs.\")\n",
    "\n",
    "parser.add_argument(\"--lr_pre\", default=5e-5, type=float,\n",
    "                    help=\"The learning rate for pre-training.\")\n",
    "\n",
    "parser.add_argument(\"--lr\", default=5e-5, type=float,\n",
    "                    help=\"The learning rate for training.\")\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--threshold\", default=0.5, type=float, help=\"Value for distinguishing confident novel samples.\")\n",
    "\n",
    "parser.add_argument(\"--num_iters_sk\", default=3, type=int, help=\"number of iters for Sinkhorn\")\n",
    "\n",
    "parser.add_argument(\"--epsilon_sk\", default=0.05, type=float, help=\"epsilon for the Sinkhorn\")\n",
    "\n",
    "parser.add_argument(\"--imb-factor\", default=1, type=float, help=\"imbalance factor of the data, default 1\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "data = Data(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage1: Multi-task Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_p = PretrainModelManager(args, data)\n",
    "manager_p.train(args, data)\n",
    "# manager_p.load_model(args)\n",
    "manager_p.evaluation(args, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage2: Pseudo-label Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = train.Manager(args, data, manager_p.model)\n",
    "manager.train(args, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "manager.model.eval()\n",
    "pred_labels = torch.empty(0, dtype=torch.long).to(manager.device)\n",
    "total_labels = torch.empty(0, dtype=torch.long).to(manager.device)\n",
    "\n",
    "for batch in data.test_dataloader:\n",
    "    batch = tuple(t.to(manager.device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "    X = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": segment_ids}\n",
    "    with torch.no_grad():\n",
    "         _, logits = manager.model(X, output_hidden_states=True)\n",
    "    labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "    pred_labels = torch.cat((pred_labels, labels))  \n",
    "    total_labels = torch.cat((total_labels, label_ids))  \n",
    "\n",
    "y_pred = pred_labels.cpu().numpy()\n",
    "y_true = total_labels.cpu().numpy()\n",
    "\n",
    "results = clustering_score(y_true, y_pred, data.known_lab)\n",
    "print('results', results)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
